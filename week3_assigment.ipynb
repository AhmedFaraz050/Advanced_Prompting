{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Assignment\n",
    "### Advanced Prompting: Zero-Shot, Few-Shot, and Chain-of-Thought (CoT)\n",
    "\n",
    "**Objective:** Compare how different prompting strategies (Zero-Shot, Few-Shot, CoT) affect reasoning performance on logic, math, and reasoning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "logic_puzzles = [\n",
    "    {\"id\": 1, \"puzzle\": \"Alice is older than Bob. Bob is older than Charlie. Who is the youngest?\", \"expected_answer\": \"Charlie\"},\n",
    "    {\"id\": 2, \"puzzle\": \"There are three boxes: one with apples, one with oranges, one mixed. All are mislabeled. You can take one fruit from one box. How do you correctly label all?\", \"expected_answer\": \"Pick from the 'mixed' box; use that to correct labels.\"},\n",
    "    {\"id\": 3, \"puzzle\": \"If all Bloops are Razzies and all Razzies are Lazzies, are all Bloops definitely Lazzies?\", \"expected_answer\": \"Yes\"}\n",
    "]\n",
    "\n",
    "math_problems = [\n",
    "    {\"id\": 1, \"problem\": \"A train travels 60 km in 1 hour. How far will it go in 4 hours?\", \"expected_answer\": \"240 km\"},\n",
    "    {\"id\": 2, \"problem\": \"What is 23 × 17?\", \"expected_answer\": \"391\"},\n",
    "    {\"id\": 3, \"problem\": \"If a pizza is cut into 8 equal slices and you eat 3, what fraction remains?\", \"expected_answer\": \"5/8\"}\n",
    "]\n",
    "\n",
    "reasoning_tasks = [\n",
    "    {\"id\": 1, \"task\": \"If John is taller than Mary, and Mary is taller than Sam, who is the tallest?\", \"expected_answer\": \"John\"},\n",
    "    {\"id\": 2, \"task\": \"A farmer has 5 cows, each produces 8 liters of milk per day. How much milk in total per day?\", \"expected_answer\": \"40 liters\"},\n",
    "    {\"id\": 3, \"task\": \"If it is raining, the ground will be wet. The ground is wet. Does it mean it rained?\", \"expected_answer\": \"Not necessarily (other causes possible).\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Simulate Model Outputs (Zero-Shot, Few-Shot, CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [\n",
    "    {\"Task\": \"Logic Puzzle #1\", \n",
    "     \"Zero-Shot\": \"Bob\", \n",
    "     \"Few-Shot\": \"Charlie\", \n",
    "     \"CoT\": \"Step 1: Alice > Bob > Charlie. Step 2: The youngest is Charlie. Answer: Charlie\"},\n",
    "    \n",
    "    {\"Task\": \"Math Problem #1\", \n",
    "     \"Zero-Shot\": \"180 km\", \n",
    "     \"Few-Shot\": \"240 km\", \n",
    "     \"CoT\": \"Step 1: 60 km in 1 hr. Step 2: Multiply by 4 → 240 km. Answer: 240 km\"},\n",
    "    \n",
    "    {\"Task\": \"Reasoning Task #3\", \n",
    "     \"Zero-Shot\": \"Yes, it rained\", \n",
    "     \"Few-Shot\": \"Not necessarily\", \n",
    "     \"CoT\": \"Step 1: If raining → ground wet. Step 2: Wet ground could have other causes (sprinklers). Step 3: So not necessarily. Answer: Not necessarily.\"}\n",
    "]\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate Using Rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = [\n",
    "    {\"Task\": \"Logic Puzzle #1\", \"Zero-Shot\": (0,0,0,2), \"Few-Shot\": (3,2,2,2), \"CoT\": (3,3,3,2)},\n",
    "    {\"Task\": \"Math Problem #1\", \"Zero-Shot\": (1,0,0,2), \"Few-Shot\": (3,2,2,2), \"CoT\": (3,3,3,2)},\n",
    "    {\"Task\": \"Reasoning Task #3\", \"Zero-Shot\": (1,1,1,2), \"Few-Shot\": (3,2,2,2), \"CoT\": (3,3,3,2)}\n",
    "]\n",
    "\n",
    "# Convert to dataframe with totals\n",
    "scores = []\n",
    "for row in evaluation:\n",
    "    scores.append({\n",
    "        \"Task\": row[\"Task\"],\n",
    "        \"Zero-Shot\": sum(row[\"Zero-Shot\"]),\n",
    "        \"Few-Shot\": sum(row[\"Few-Shot\"]),\n",
    "        \"CoT\": sum(row[\"CoT\"])\n",
    "    })\n",
    "\n",
    "df_scores = pd.DataFrame(scores)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Reflection\n",
    "\n",
    "- **Zero-Shot** struggled the most, often giving incomplete or wrong answers (scores mostly low).  \n",
    "- **Few-Shot** improved performance by giving the model context/examples, leading to more correct answers.  \n",
    "- **CoT** consistently gave the best results: not only correct but also explained step by step → highest clarity and completeness.  \n",
    "\n",
    "➡️ **Conclusion**: Chain-of-Thought prompting was the most reliable strategy for reasoning tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
